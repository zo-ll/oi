#!/bin/bash
#
# oi - One-command LLM chat interface for llama.cpp
# Interactive model selection and chat launcher
#
VERSION="1.0.0"

# Don't use set -e as it causes issues with the interactive menu loop

# Use absolute path to installed location
SCRIPT_DIR="${HOME}/.local/share/oi"
LIB_DIR="${SCRIPT_DIR}/lib"

# Configuration
LLAMA_CPP_DIR="${HOME}/.local/share/oi/llama.cpp"
MODELS_DIR="${LLAMA_CPP_DIR}/models"
LLAMA_CLI="${LLAMA_CPP_DIR}/build/bin/llama-cli"
DEFAULT_QUANT="Q4_K_M"
DEFAULT_CONTEXT=4096
CACHE_DIR="${LIB_DIR}/cache"
CACHE_FILE="${CACHE_DIR}/hf_models.json"
CACHE_TTL=86400  # 24 hours
HF_ORGS="Qwen meta-llama microsoft google mistralai bartowski unsloth MaziyarPanahi"

# Colors for output using ANSI escape sequences
RED=$'\033[0;31m'
GREEN=$'\033[0;32m'
YELLOW=$'\033[1;33m'
BLUE=$'\033[0;34m'
CYAN=$'\033[0;36m'
NC=$'\033[0m' # No Color

# Source modules
source "${LIB_DIR}/ui.sh"
source "${LIB_DIR}/hardware.sh"
source "${LIB_DIR}/models.sh"
source "${LIB_DIR}/download.sh"
source "${LIB_DIR}/chat.sh"

# Main function
main() {
    # Parse arguments
    local model_id=""
    local quant="$DEFAULT_QUANT"
    local context="$DEFAULT_CONTEXT"
    local threads="$(nproc)"
    local list_mode=""
    local download_path=""

    while [[ $# -gt 0 ]]; do
        case "$1" in
            -m|--model)
                model_id="$2"
                shift 2
                ;;
            -q|--quant)
                quant="$2"
                shift 2
                ;;
            -c|--context)
                context="$2"
                shift 2
                ;;
            -t|--threads)
                threads="$2"
                shift 2
                ;;
            -r|--refresh)
                rm -f "$CACHE_FILE"
                fetch_hf_models
                shift
                ;;
            -l|--list)
                list_mode="all"
                shift
                ;;
            -i|--installed)
                list_mode="installed"
                shift
                ;;
            -x|--remove)
                remove_file="$2"
                shift 2
                ;;
            -d|--download)
                download_path="$2"
                shift 2
                ;;
            -h|--hardware)
                check_llama_cpp
                show_hardware
                exit 0
                ;;
            --version)
                echo "oi version $VERSION"
                exit 0
                ;;
            --help)
                show_help
                exit 0
                ;;
            *)
                echo -e "${RED}Unknown option: $1${NC}"
                show_help
                exit 1
                ;;
        esac
    done

    # Check llama.cpp setup
    check_llama_cpp

    # Handle list modes
    if [ "$list_mode" = "all" ]; then
        list_models
        exit 0
    elif [ "$list_mode" = "installed" ]; then
        list_installed_models
        exit 0
    fi

    # Handle model removal
    if [ -n "$remove_file" ]; then
        remove_model "$remove_file"
        exit $?
    fi

    # Handle custom download
    if [ -n "$download_path" ]; then
        download_custom "$download_path"
        exit $?
    fi

    # Main flow
    if [ -n "$model_id" ]; then
        # Direct model selection
        launch_chat "$model_id" "$quant" "$context" "$threads"
    else
        # Interactive mode
        while true; do
            selected=$(interactive_select)
            local select_status=$?

            if [ $select_status -eq 0 ]; then
                # User selected a model number
                launch_chat "$selected" "$quant" "$context" "$threads"
                echo -e "\n${GRAY}Returning to model selection...${NC}\n"
            elif [ $select_status -eq 2 ]; then
                # User selected Q to quit
                exit 0
            fi
            # Otherwise (status 1), loop continues for L or H selection
        done
    fi
}

# Run main
main "$@"
